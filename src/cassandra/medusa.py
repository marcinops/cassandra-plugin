#
# Copyright (c) 2020 by Delphix. All rights reserved.
#

import os
import json
import re
import itertools
from datetime import datetime
from dlpx.virtualization.platform.exceptions import UserError
from controller.helper import execute_bash

import logging
logger = logging.getLogger(__name__)

class Medusa(object):
    """
    Backup vendor class
    Support for backup generated by 
    https://github.com/thelastpickle/cassandra-medusa
    """
    def __init__(self, node_obj):
        self.__node_obj = node_obj
        self.config = node_obj.config

    @property
    def node_obj(self):
        return self.__node_obj

    def tokens_for_new_cluster(self):
        return (10,'')

    def get_initial_token(self):
        # add code to select between functions
        return self.tokens_for_new_cluster()


    def load_meta(self, path, nonsystemks):
        # load manifiests and return a list
        # of keyspaces with locations of those files as dicts
        # [ { "keyspace":"ks1", "tablename":"foo", "path": "s3", "size":"bytes"}, {   } ]


        logger.debug("load meta started")
        
        local_schema_file = os.path.join(self.node_obj.node_dir, "manifest.json")
        ret = execute_bash(self.config.connection, "copy_file_s3", src=path, dest=local_schema_file)
        if ret.exit_code != 0:
            raise UserError(
                'Failed to find metadata file',
                'Make sure the user has appropriate permissions and path is correct',
                '{}\n{}'.format(ret.stdout, ret.stderr))

        ret = execute_bash(self.config.connection, "read_file", file="{}".format(local_schema_file))
        if ret.exit_code == 0:
            # OK - we need to parse stderr to get rid of Already exist errors and code=2100
            try:
                manifest = json.loads(ret.stdout)
            except ValueError:
                raise UserError(
                    'Failed to parse manifest',
                    'Make sure the user has appropriate permissions and path is correct',
                    '{}\n{}'.format(ret.stdout, ret.stderr))          
        else:
            raise UserError(
                'Failed to copy manifest',
                'Make sure the user has appropriate permissions and path is correct',
                '{}\n{}'.format(ret.stdout, ret.stderr))          

        return self.process_meta_file(manifest, nonsystemks)


    def standarize_manifest(self, d):
        """
        Process a manifest file and return a standard structure designed for plugin
        dict:
            keyspace: keyspace name
            table: column family name
            size: size of all files for column family (table)
            nodename: name of the node
            files: list of dict:
                path: path to backup of SStable file
                md5: md5 of SStable file 
        """
        S3bucket = self.config.staged_source.parameters.s3bucket

        if len(d["objects"])>0:
            size = sum(x["size"] for x in d["objects"])
            dfreg = re.compile(r'(.*)-Data\.db$')
            datafiles = [ x for x in d["objects"] if re.search(dfreg, x["path"]) ]
            logger.debug("Data files")
            logger.debug(datafiles)
            if len(datafiles) < 1:
                raise UserError(
                    'Failed find data file in manifest {}'.format(str(d["objects"])),
                    'xxxx')   


            nodereg = re.match(r'^([^/]*).*-Data\.db$', datafiles[0]["path"])
            nodename = nodereg.group(1)
            logger.debug("Nodename {}".format(nodename)) 

            files = []
            for df in datafiles:
                f = re.match(dfreg, df["path"])
                s3path = "s3://{}/{}".format(S3bucket, f.group(1))
                md5 = df["MD5"]
                logger.debug("FULL {}".format(df["path"]))
                logger.debug("File {}".format(s3path))
                files.append({ "path": s3path, "md5": md5})
        else:
            size = 0
            path = ''
            md5 = 0
            files = []
            nodename = ''

        return { "files": files, "keyspace": d["keyspace"], "table": d["columnfamily"], "size": size, "nodename": nodename }


    def process_meta_file(self, manifest, nonsystemks):
        # process maniferst
        # of keyspaces with locations of those files as dicts
        # [ { "keyspace":"ks1", "tablename":"foo", "files":[ { "path":"s3....", "size":"bytes"} ] } ]

        logger.debug("process meta started")

        manifests_to_process = [ x for x in manifest if x["keyspace"] in nonsystemks ]

        # rename dicts
        #n = [ {y.replace("columnfamily","table"): u for y,u in x.items() } for x in manifests_to_process ]
        #keyspace_list = [ {y.replace("objects","files"): u for y,u in x.items() } for x in n ] 

        return map(self.standarize_manifest, manifests_to_process)


    def find_manifest(self):
        # find manifest file

        file_name = "manifest.json"
        backup_name = self.config.backup_name
        S3bucket = self.config.staged_source.parameters.s3bucket
        s3name = "*/{}/*/{}".format(backup_name, file_name)
        bucket = "s3://{}".format(S3bucket)
        ret = execute_bash(self.config.connection, "find_s3_file", file=s3name, bucket=bucket, local=self.node_obj.node_dir)
        loc = []
        if ret.exit_code == 0:
            # OK - we need to parse stdout to return locations

            loc = [ m.group(1) for x in ret.stdout.split('\n') for m in [re.search(r'(s3://\S*)', x)]  if m ]
            logger.debug("loc: {}".format(loc))
        else:
            raise UserError(
                'Failed to find a manifest in backup {}'.format(backup_name),
                'Make sure the user has appropriate permissions and path is correct',
                '{}\n{}'.format(ret.stdout, ret.stderr))    

        return loc


    def find_schema(self):
        logger.debug(str(self.config.staged_source))
        schema_file_name = self.config.staged_source.parameters.schema_file_name
        backup_name = self.config.backup_name
        S3bucket = self.config.staged_source.parameters.s3bucket
        s3name = "*/{}/*/{}".format(backup_name, schema_file_name)
        bucket = "s3://{}".format(S3bucket)
        ret = execute_bash(self.config.connection, "find_s3_file", file=s3name, bucket=bucket, local=self.node_obj.node_dir)
        loc = []
        if ret.exit_code == 0:
            # OK - we need to parse stdout to return locations
            loc = [ m.group(1) for x in ret.stdout.split('\n') for m in [re.search(r'(s3://\S*)', x)]  if m ]
            logger.debug("loc: {}".format(loc))
        else:
            raise UserError(
                'Failed to copy manifest',
                'Make sure the user has appropriate permissions and path is correct',
                '{}\n{}'.format(ret.stdout, ret.stderr))    

        if len(loc)<1:
            raise UserError(
                'Schema file {} not found in backup {}'.format(schema_file_name, s3name),
                'Make sure the user has appropriate permissions and path is correct',
                '{}\n{}'.format(ret.stdout, ret.stderr))      

        # only one schema file needs to be downloaded - all should be same in same backup
        return loc[0]


    def list_backups(self):
        """
        list all avaliable backups with types
        Return ordered list of tuples { backupname, backup time, differential}
        """
        logger.debug(str(self.config.staged_source))
        schema_file_name = self.config.staged_source.parameters.schema_file_name
        S3bucket = self.config.staged_source.parameters.s3bucket
        finddiff = "index/backup_index/*/differential*"
        findall = "index/backup_index/*/started*"
        bucket = "s3://{}".format(S3bucket)
        

        rebackup = re.compile(r'.*s3://[^/]*/index/backup_index/([^/]*)/started.*?([\d]*)\.timestamp')
        rediff = re.compile(r'.*s3://[^/]*/index/backup_index/([^/]*)/differential.*?')

        # list only differential first
        backup_diff = {}
        ret = execute_bash(self.config.connection, "find_s3_file", file=finddiff, bucket=bucket, local=self.node_obj.delphix_dir)
        if ret.exit_code == 0:
            backup_diff = { gr.group(1): True for l in ret.stdout.split("\n") for gr in [re.match(rediff, l)] if gr }
        else:
            raise UserError(
                'Failed to list differential backups',
                'Make sure the user has appropriate permissions and path is correct',
                '{}\n{}'.format(ret.stdout, ret.stderr))    


        logger.debug(backup_diff)

        # list all backups and covert into list
        backup_list_all = []
        ret = execute_bash(self.config.connection, "find_s3_file", file=findall, bucket=bucket, local=self.node_obj.delphix_dir)
        if ret.exit_code == 0:
            for l in ret.stdout.split("\n"):
                gr = re.match(rebackup, l)
                if gr:
                    if gr.group(1) in backup_diff:
                        diff = True
                    else:
                        diff = False

                    backup_list_all.append((gr.group(1), datetime.fromtimestamp(int(gr.group(2))).strftime('%Y-%m-%d %H:%M:%S'), diff))

            backup_list = []

            for k,g in itertools.groupby(backup_list_all, lambda x: x[0] ):
                line = list(g)
                res = max(line, key = lambda x: x[1])
                backup_list.append(res)

            backup_list =  sorted(backup_list, key=lambda x: x[1])
            logger.debug(backup_list)
    
        else:
            raise UserError(
                'Failed to list backups',
                'Make sure the user has appropriate permissions and path is correct',
                '{}\n{}'.format(ret.stdout, ret.stderr))    


        return backup_list






